{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f0827e",
   "metadata": {},
   "source": [
    "## Research Questions Focus Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b22300",
   "metadata": {},
   "source": [
    "- Note: Please endeavour to explicitly comment your codes and properly document whichever functions created so as to help other collaborators learn from your codes quicker. Remember that the project is also a learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a7b59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd16e5e",
   "metadata": {},
   "source": [
    "### Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99e256bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6241e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caca0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, random, re, os, os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Data Cleaning \n",
    "from langdetect import detect\n",
    "from langdetect.detector import LangDetectException\n",
    "from googletrans import Translator\n",
    "\n",
    "#Datas Encoding\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Sentiment Analysis\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import model_selection\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, SimpleRNN, LSTM, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Topic Modelling\n",
    "import glob, pprint, spacy\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis, pyLDAvis.gensim_models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141990bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus, Dictionary etc declaration\n",
    "\n",
    "wv = KeyedVectors.load('./Election-Campaign-Application-Phase2-Implementation/app/corpus/word2vec-google-news-300')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f2960",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413ba241",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "tweets = pd.read_csv(\"citizensvoice_dataset.csv\", index_col=0, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f260e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>tweet</th>\n",
       "      <th>loca_tion</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>discourse_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-25T23:44:56+00:00</td>\n",
       "      <td>Tinubu Is An Emperor; Buhari, Osinbajo, Governors Begged Him To Forgive Ambode But He Refused âDele Momodu | Sahara Reporters https://t.co/mO1zWgXQxn</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>negative</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-25T23:37:40+00:00</td>\n",
       "      <td>Dear @PeterObi please stop putting our future at risk. \\r\\nYou are the only reason I still believe in Nigeria. \\r\\nMy vote is for you https://t.co/nKhLhzrV8H</td>\n",
       "      <td>Lagos, Nigeria</td>\n",
       "      <td>positive</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-25T23:31:19+00:00</td>\n",
       "      <td>Wike pointed to how the PDP presidential candidate, Alhaji Atiku Abubakar picked people from Rivers State as members of the presidential campaign council without any input from him.\\r\\n\\r\\nhttps://t.co/H2cicBJlu1</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>indifferent</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-25T23:03:57+00:00</td>\n",
       "      <td>@fkeyamo @apc_lagos https://t.co/KrKdTG8prX</td>\n",
       "      <td>Ogun, Nigeria</td>\n",
       "      <td>indifferent</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-27T23:59:39+00:00</td>\n",
       "      <td>PDP is in total chaos in Ogun, dead in Lagos, Oyo PDP refusing to work for Atiku, the leaders in Ekiti and Ondo are refusing to mount a challenge, only in Osun does the party have a deem hope. https://t.co/w3r3dmSa0i</td>\n",
       "      <td>Ogun, Nigeria</td>\n",
       "      <td>negative</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                time_created  \\\n",
       "0  2022-10-25T23:44:56+00:00   \n",
       "1  2022-10-25T23:37:40+00:00   \n",
       "2  2022-10-25T23:31:19+00:00   \n",
       "3  2022-10-25T23:03:57+00:00   \n",
       "4  2022-10-27T23:59:39+00:00   \n",
       "\n",
       "                                                                                                                                                                                                                      tweet  \\\n",
       "0                                                                   Tinubu Is An Emperor; Buhari, Osinbajo, Governors Begged Him To Forgive Ambode But He Refused âDele Momodu | Sahara Reporters https://t.co/mO1zWgXQxn   \n",
       "1                                                             Dear @PeterObi please stop putting our future at risk. \\r\\nYou are the only reason I still believe in Nigeria. \\r\\nMy vote is for you https://t.co/nKhLhzrV8H   \n",
       "2      Wike pointed to how the PDP presidential candidate, Alhaji Atiku Abubakar picked people from Rivers State as members of the presidential campaign council without any input from him.\\r\\n\\r\\nhttps://t.co/H2cicBJlu1   \n",
       "3                                                                                                                                                                               @fkeyamo @apc_lagos https://t.co/KrKdTG8prX   \n",
       "4  PDP is in total chaos in Ogun, dead in Lagos, Oyo PDP refusing to work for Atiku, the leaders in Ekiti and Ondo are refusing to mount a challenge, only in Osun does the party have a deem hope. https://t.co/w3r3dmSa0i   \n",
       "\n",
       "        loca_tion    sentiment discourse_area  \n",
       "0         Nigeria     negative      unrelated  \n",
       "1  Lagos, Nigeria     positive      unrelated  \n",
       "2         Nigeria  indifferent      unrelated  \n",
       "3   Ogun, Nigeria  indifferent      unrelated  \n",
       "4   Ogun, Nigeria     negative      unrelated  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca1758",
   "metadata": {},
   "source": [
    "### Dataset Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65cf86",
   "metadata": {},
   "source": [
    "+ Using one of our Research Questions to guide the data wrangling. If we consider a simple question of \"What is being said about Peter Obi?\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c9c545-188f-4199-8559-7baaaca99fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.tweet = tweets.tweet.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e8a319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>tweet</th>\n",
       "      <th>loca_tion</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>discourse_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-25T23:44:56+00:00</td>\n",
       "      <td>tinubu is an emperor; buhari, osinbajo, governors begged him to forgive ambode but he refused âdele momodu | sahara reporters https://t.co/mo1zwgxqxn</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>negative</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                time_created  \\\n",
       "0  2022-10-25T23:44:56+00:00   \n",
       "\n",
       "                                                                                                                                                     tweet  \\\n",
       "0  tinubu is an emperor; buhari, osinbajo, governors begged him to forgive ambode but he refused âdele momodu | sahara reporters https://t.co/mo1zwgxqxn   \n",
       "\n",
       "  loca_tion sentiment discourse_area  \n",
       "0   Nigeria  negative      unrelated  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165e36b",
   "metadata": {},
   "source": [
    "#### 1. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf2338",
   "metadata": {},
   "source": [
    "+ Cleaning: Clean any element of the dataset that might affect our NLP algorithm.\n",
    "        - Remove \"/n\", links and emojis.\n",
    "        - Replace &amp; with and.\n",
    "+ In future versions of this project, we might try to analyse some of these element, like the emojis as they could be essential for our sentimental analysis, but for now we keep it simple and focus on the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5096836d-3e8a-401b-9b6e-42f478507fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode for emojis\n",
    "emojis = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b09844-1e0e-44ec-8f0d-efe9a9d905e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Function to clean tweet by:\n",
    "    - Changing '&' sign to and\n",
    "    - Removing newlines, carriage returns, links, emojis, handles, hashtags and punctuations\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): The tweet\n",
    "    \"\"\"      \n",
    "            \n",
    "    tweet = re.sub(\"@[^\\s]+\", \"\",tweet) # removes handles\n",
    "    tweet = re.sub(\"\\n\", \" \", tweet) # remove newlines\n",
    "    tweet = re.sub(\"\\r\", \"\", tweet) # remove carriage returns\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet) # removes links\n",
    "    tweet = re.sub(emojis, \"\", tweet) # remove emojis\n",
    "    tweet = re.sub(r\"#(\\w+)\", \"\", tweet) # remove hashtags\n",
    "    tweet = re.sub(\"&\", \"and\", tweet) # changes & sign to and\n",
    "    tweet = re.sub(r\"[^\\w\\s\\@]\",\"\",tweet) # removes punctuation\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193b6952-289f-4aad-bc98-e3faeb23ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"clean_tweet\"] = tweets.tweet.apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335386f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16854</th>\n",
       "      <td>apc inaugurates presidential campaign council in canada https://t.co/wmrqdmnw7h</td>\n",
       "      <td>apc inaugurates presidential campaign council in canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36557</th>\n",
       "      <td>@aidelomo_happy @mysteriousdoct3 @shehusani that is ur id card for sure. you can keep piling till next year december no wahala, but one thing you shud know is that your principal can't win the election, simple as apc</td>\n",
       "      <td>that is ur id card for sure you can keep piling till next year december no wahala but one thing you shud know is that your principal cant win the election simple as apc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>@mahdeeus @rbleipzig_en see what vini cause us</td>\n",
       "      <td>see what vini cause us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10129</th>\n",
       "      <td>as tinubu dey lose, peter obi dey gain. north central is locked down for po.\\r\\n\\r\\nfirst lady arise tv access bank osinbajo tinubu igbo yorubas ambode muslim-muslim funsho williams yaba https://t.co/acyfev0l5n</td>\n",
       "      <td>as tinubu dey lose peter obi dey gain north central is locked down for po  first lady arise tv access bank osinbajo tinubu igbo yorubas ambode muslimmuslim funsho williams yaba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9482</th>\n",
       "      <td>@peterobi our incoming presidentð</td>\n",
       "      <td>our incoming presidentð</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                          tweet  \\\n",
       "16854                                                                                                                                           apc inaugurates presidential campaign council in canada https://t.co/wmrqdmnw7h   \n",
       "36557  @aidelomo_happy @mysteriousdoct3 @shehusani that is ur id card for sure. you can keep piling till next year december no wahala, but one thing you shud know is that your principal can't win the election, simple as apc   \n",
       "693                                                                                                                                                                              @mahdeeus @rbleipzig_en see what vini cause us   \n",
       "10129        as tinubu dey lose, peter obi dey gain. north central is locked down for po.\\r\\n\\r\\nfirst lady arise tv access bank osinbajo tinubu igbo yorubas ambode muslim-muslim funsho williams yaba https://t.co/acyfev0l5n   \n",
       "9482                                                                                                                                                                                       @peterobi our incoming presidentð   \n",
       "\n",
       "                                                                                                                                                                            clean_tweet  \n",
       "16854                                                                                                                           apc inaugurates presidential campaign council in canada  \n",
       "36557          that is ur id card for sure you can keep piling till next year december no wahala but one thing you shud know is that your principal cant win the election simple as apc  \n",
       "693                                                                                                                                                              see what vini cause us  \n",
       "10129  as tinubu dey lose peter obi dey gain north central is locked down for po  first lady arise tv access bank osinbajo tinubu igbo yorubas ambode muslimmuslim funsho williams yaba  \n",
       "9482                                                                                                                                                            our incoming presidentð  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[[\"tweet\", \"clean_tweet\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c9d163c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_created</th>\n",
       "      <th>tweet</th>\n",
       "      <th>loca_tion</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>discourse_area</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-25T23:44:56+00:00</td>\n",
       "      <td>tinubu is an emperor; buhari, osinbajo, governors begged him to forgive ambode but he refused âdele momodu | sahara reporters https://t.co/mo1zwgxqxn</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>negative</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>tinubu is an emperor buhari osinbajo governors begged him to forgive ambode but he refused âdele momodu  sahara reporters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                time_created  \\\n",
       "0  2022-10-25T23:44:56+00:00   \n",
       "\n",
       "                                                                                                                                                     tweet  \\\n",
       "0  tinubu is an emperor; buhari, osinbajo, governors begged him to forgive ambode but he refused âdele momodu | sahara reporters https://t.co/mo1zwgxqxn   \n",
       "\n",
       "  loca_tion sentiment discourse_area  \\\n",
       "0   Nigeria  negative      unrelated   \n",
       "\n",
       "                                                                                                                 clean_tweet  \n",
       "0  tinubu is an emperor buhari osinbajo governors begged him to forgive ambode but he refused âdele momodu  sahara reporters  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d801b",
   "metadata": {},
   "source": [
    "#### 2. Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce587cc0",
   "metadata": {},
   "source": [
    "+ Convert all tweets to lower case.\n",
    "+ Translate: Here we convert all non-english tweet to English for a smooth and uniform analysis\n",
    "        - If text is not in English convert to English (using google translate or any other suitable library or api).\n",
    "+ This is much time saving as systems have already been develped for such translation, instead of us having to develop our NLP kit or algorithm for each language used in the Nigerian twitter space. We can simply translate with the already existing systems and then analyse with the already trained systems.\n",
    "+ In future versions of this project, we could look into developing our own custom NLP algorithm and kit tailored to our own native languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bb23384",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "765815e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tweet(tweet):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to translate non-english tweets by checking if the tweet is non-english\n",
    "    and translating it using the googletrans library.\n",
    "    It returns the unchanaged tweet if it is in english, a space, empty or numeric.\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): The tweet\n",
    "    \"\"\" \n",
    "    try:\n",
    "        if detect(tweet) != \"en\":\n",
    "            return translator.translate(tweet).text\n",
    "        return tweet\n",
    "    \n",
    "    except LangDetectException:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af108d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_english(tweet):\n",
    "    \"\"\"\n",
    "    Function that finds non-english tweets.\n",
    "    \n",
    "    Parameters:\n",
    "        tweet (string): The tweet\n",
    "    \"\"\" \n",
    "    try:\n",
    "        if detect(tweet) != \"en\":\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    except LangDetectException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1688fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tweets[\"translated_tweet\"] = tweets.clean_tweet.apply(translate_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf7d336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16min 1s\n",
      "Wall time: 16min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#non_english = tweets.clean_tweet.apply(find_non_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets[non_english][[\"clean_tweet\", \"translated_tweet\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d5e4fa",
   "metadata": {},
   "source": [
    "#### 3. Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046da19",
   "metadata": {},
   "source": [
    "+ Filtering: Filter for tweets directed at Peter Obi, based on the following rules:\n",
    "        - Peter Obi's handle appears first in tweet.\n",
    "        - Peter Obi's name (not handle) appears any where in tweet.\n",
    "        - Peter Obi's handle appears in tweet but not after another handle.\n",
    "+ These rule help us focus the results on tweet directed to or about Peter Obi, instead of including tweets that could simply be replies to other twitter users under Peter Obi's tweet or replies to other twitter users who posted a tweet with Peter Obi's handle in it.\n",
    "+ These rules were derived from domain knowledge of the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95dfccf5-5df3-4eec-a232-07152b161906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweet(tweet, handle, mentions):\n",
    "    \"\"\"\n",
    "    Function that filters tweet Filter for tweets directed at handle, based on the following rules:\n",
    "    - The handle appears first in tweet.\n",
    "    - The handle appears in tweet but not after another handle.\n",
    "    - The person is mentioned any where in tweet based on the list of metions.\n",
    "\n",
    "    Parameters:\n",
    "        tweet (string): The tweet\n",
    "        handle (string): The username of the subject to be filtered for should start with '@'\n",
    "        mentions (list): A list of other ways the subject could be mentioned in the text\n",
    "    \"\"\"\n",
    "\n",
    "    # Split text into tokens\n",
    "    tokens = tweet.split()\n",
    "\n",
    "    # Check for tokens that have the handle\n",
    "    indices = [i for i, token in enumerate(tokens) if token == handle]\n",
    "\n",
    "    for index in indices:\n",
    "\n",
    "        # Checks if the handle appear first\n",
    "        if index==0:\n",
    "            return True\n",
    "\n",
    "        # Checks if the another handle appears before it\n",
    "        if not tokens[index-1].startswith(\"@\"):\n",
    "            return True\n",
    "\n",
    "    # Checks if the person is mentioned anywhere in the tweet\n",
    "    for mention in mentions:\n",
    "        if mention in tweet:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "552c3ca9-bd79-4b62-86a8-f067fab74f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#po_tweets = tweets[tweets.tweet.apply(filter_tweet, handle=\"@peterobi\", mentions=[\"peter obi\", \" peterobi\", \" po \"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601a4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#po_tweets.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a3a1916-3a08-4f35-a682-b4dd9dfeba4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9134"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(po_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52506638",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df823f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2097, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset for Sentiment Analysis\n",
    "senttweets = tweets[[\"clean_tweet\",\"sentiment\"]][~tweets.sentiment.isnull()]\n",
    "senttweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df70c62",
   "metadata": {},
   "source": [
    "### Dataset Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103ff8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696d6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "sent = le.fit_transform(senttweets['sentiment'])\n",
    "text = senttweets['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1daf2af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ccccfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       negative\n",
       "1       positive\n",
       "2    indifferent\n",
       "3    indifferent\n",
       "4       negative\n",
       "5       negative\n",
       "6    indifferent\n",
       "7       positive\n",
       "8       negative\n",
       "9       negative\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senttweets['sentiment'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bbe68de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 505, 2: 502, 0: 1090})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    " \n",
    "collections.Counter(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa18eb2",
   "metadata": {},
   "source": [
    "- 1. Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "57f5eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def bagofwords_preprocessing(text, label):\n",
    "    \"\"\"This function removes stop words and lemmatizes the dataset, after which it uses the most frequent\n",
    "    words to create a feature set that it will use to generate the encoded dataset with binary values.\"\"\"\n",
    "    \n",
    "    #remove stope words from text\n",
    "    processed = text.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n",
    "\n",
    "    #remove word stems using a Porter stemmer\n",
    "    processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))\n",
    "\n",
    "    #creating a bag-of-words\n",
    "    all_words = []\n",
    "\n",
    "    for message in processed:\n",
    "        words = word_tokenize(message)\n",
    "        for w in words:\n",
    "            all_words.append(w)\n",
    "\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    #This bag of words will help us selects the words that will be used for our features.\n",
    "\n",
    "    #use the 1500 most common words as features\n",
    "    word_features = list(all_words.keys())[:1500]\n",
    "\n",
    "    #This merges our pre-processed data with it label\n",
    "    messages = zip(processed, label)\n",
    "    \n",
    "    return messages, word_features\n",
    "\n",
    "def find_features(message, word_features):\n",
    "    \"\"\"This function basically picks a text and creates a row,\n",
    "    by checking if it contains any of the common words or not.\"\"\"\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd431804",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages, word_features = bagofwords_preprocessing(text, sent)\n",
    "\n",
    "%%time\n",
    "#building dataset using our function for generating rows using our word features\n",
    "dataset_1 = [(find_features(text, word_features), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d8ed3",
   "metadata": {},
   "source": [
    "- 2. Sentence Character-Encoder (Custom Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "69d43b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def custom_dictionary():\n",
    "    \"\"\"Dictionary to be used for the sentence-character vector encoding.\"\"\"\n",
    "\n",
    "    alphabets = [' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "    alpha_codes = ['%.2f' % elem for elem in np.linspace(0, 1, 27)]\n",
    "    code_dict = dict(zip(alphabets, alpha_codes))\n",
    "    \n",
    "    return code_dict\n",
    "\n",
    "def custom_sentencencoder(text):\n",
    "    \"\"\"Vector Senctence Encoding for each tweet.\n",
    "    \n",
    "    This approach does not omit stop words, nor lemmatize so as to retain all information within the sentence. \n",
    "    \n",
    "    However the context of each word is lost, therefore if the same sentence is constructed using different words, \n",
    "    the algorithm might not recognize the similarity.\"\"\"\n",
    "\n",
    "    #Creating dataframe with 300 features meant to represent each character in the sentence\n",
    "    twitterchar_len = 300\n",
    "    cols = ['char_'+str(index) for index in range(1,twitterchar_len+1)]\n",
    "    dataset = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    for txt in text:\n",
    "        instance = []\n",
    "        for index in range(dataset.shape[1]):\n",
    "            if index < len(txt):\n",
    "                if txt[index] in code_dict.keys():\n",
    "                    instance.append(code_dict[txt[index]])\n",
    "                else:\n",
    "                    instance.append(code_dict[' '])\n",
    "            else:\n",
    "                #Vector Padding\n",
    "                instance.append(code_dict[' '])\n",
    "        dataset.loc[len(dataset.index)] = instance\n",
    "        \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5de32128",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dict = custom_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ba63a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset_2 = custom_sentencencoder(text)\n",
    "dataset_2 = dataset_2.astype(float)\n",
    "\n",
    "training2 = dataset_2.to_numpy()\n",
    "label2 = np.array(sent)\n",
    "label2 = to_categorical(label2, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9885f7",
   "metadata": {},
   "source": [
    "- 3. Sentence Word-Encoder (GENSIM + PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6761862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def sent_vect3(sentence):\n",
    "    \"\"\"This function tokenizes each text and encodes each word in each text with it's vector representation\n",
    "    in the word2vec-google-news-300 GENSIM dictionary, after which it reduces the size of each word in the\n",
    "    text using Principal Component Analysis (PCA).\"\"\"\n",
    "    \n",
    "    word_token = word_tokenize(sentence)\n",
    "    sample_vector = np.array([wv[word] for word in word_token if word in wv.index_to_key])\n",
    "    if sample_vector.shape[0] > 0:\n",
    "        sample_vector = pca.fit_transform(sample_vector)\n",
    "        sample_vector = sample_vector.T.tolist()[0]\n",
    "    else:\n",
    "        sample_vector = [0.0]\n",
    "        \n",
    "    return sample_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3816c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "220b8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset_3 = text.apply(sent_vect3)\n",
    "dataset_3 = pd.DataFrame(dataset_3.tolist())\n",
    "dataset_3.fillna(0, inplace=True)\n",
    "\n",
    "training3 = dataset_3.to_numpy()\n",
    "label3 = np.array(sent)\n",
    "label3 = to_categorical(label3, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ad506",
   "metadata": {},
   "source": [
    "- 4. Sentence Word-Encoder (GENSIM + Avg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f48682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def sent_vect4(sentence):\n",
    "    \"\"\"This function tokenizes each text and encodes each word in each text with it's vector representation\n",
    "    in the word2vec-google-news-300 GENSIM dictionary, after which it reduces the size of each word in the\n",
    "    text by replacing their respective vectors with the average values of each vector.\"\"\"\n",
    "    \n",
    "    word_token = word_tokenize(sentence)\n",
    "    sample_vector = np.array([wv[word] for word in word_token if word in wv.index_to_key])\n",
    "    if sample_vector.shape[0] > 0:\n",
    "        sample_vector = sample_vector.mean(axis=1)\n",
    "        sample_vector = sample_vector.tolist()\n",
    "    else:\n",
    "        sample_vector = [0.0]\n",
    "    return sample_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aae51aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 29s\n",
      "Wall time: 6min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_4 = text.apply(sent_vect4)\n",
    "dataset_4 = pd.DataFrame(dataset_4.tolist())\n",
    "dataset_4.fillna(0, inplace=True)\n",
    "\n",
    "training4 = dataset_4.to_numpy()\n",
    "label4 = np.array(sent)\n",
    "label4 = to_categorical(label4, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f17260",
   "metadata": {},
   "source": [
    "- 5. Sentence Word-Encoder (GENSIM to Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38b93f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def sent_vect5(series):\n",
    "    \"\"\"This function tokenizes each text and encodes each word in each text with it's vector representation\n",
    "    in the word2vec-google-news-300 GENSIM dictionary.\n",
    "    \n",
    "    This nested list/array will later be converted into a tensor, and fed directly into an RNN\"\"\"\n",
    "    \n",
    "    shape = series.shape[0]\n",
    "    series = list(series.values)\n",
    "    array = []\n",
    "    pad_array = np.zeros(300)\n",
    "    for i in range(shape):\n",
    "        word_token = word_tokenize(series[i])\n",
    "        sample_vector = np.array([list(wv[word]) for word in word_token if word in wv.index_to_key])\n",
    "        if sample_vector.shape[0] > 0:\n",
    "            deficit = 50-sample_vector.shape[0]\n",
    "            for i in range(deficit):\n",
    "                sample_vector = np.vstack((sample_vector, pad_array))\n",
    "        else:\n",
    "            sample_vector = np.zeros((50, 300))\n",
    "        array.append(sample_vector.tolist())\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c9e90eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 40s\n",
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_5 = sent_vect5(text)\n",
    "\n",
    "training5 = np.array(dataset_5)\n",
    "label5 = np.array(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422766bd",
   "metadata": {},
   "source": [
    "- 6. Word Encoder Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ba7b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=1000, lower=True)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "#adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c70c7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 26.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maxlen = 50 #we set the maximum size of each list to 100\n",
    "\n",
    "training6 = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "label6 = np.array(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba020d",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e985a16",
   "metadata": {},
   "source": [
    "- National"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dd44615",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {'model1':[],'model2':[],'model2_5':[],'model3':[],'model3_5':[],'model4':[],'model4_5':[],'model5':[],'model6':[]}\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61853ee0",
   "metadata": {},
   "source": [
    "- Senitment Method 1 (with Dataset 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "698d7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training1, testing1 = model_selection.train_test_split(dataset_1, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "86e67efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a model list, just as it is done when creating a pipeline.\n",
    "names = ['Naive Bayes']\n",
    "classifier = [MultinomialNB()]\n",
    "\n",
    "models = dict(zip(names, classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ae73a51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(MultinomialNB())>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = SklearnClassifier(models['Naive Bayes'])\n",
    "model1.train(training1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d4852848",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model1'].append(nltk.classify.accuracy(model1, testing1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "baabce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./models/sentmodel1.pkl\"\n",
    "pickle.dump(model1, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5617a",
   "metadata": {},
   "source": [
    "- Senitment Method 2 (with Dataset 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cfc6f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Dense(units=50, input_shape=(training2.shape[1],), activation='relu'),\n",
    "    Dense(units=100, activation='relu'),\n",
    "    Dense(units=100, activation='relu'),\n",
    "    Dense(units=num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5165d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist2 = model2.fit(x=training2, y=label2, validation_split=0.2, batch_size=50, epochs=epochs, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "28caa11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model2'].append(hist2.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "eaa5c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/sentmodel2.h5') is False:\n",
    "    model2.save('./models/sentmodel2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6e1fded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset2 combined with SimpleRNN\n",
    "\n",
    "training2_5 = np.array(training2).reshape((training2.shape[0]), training2.shape[1], 1)\n",
    "\n",
    "label2_5 = to_categorical(np.array(sent), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c56448b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_5 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training2.shape[1], 1), return_sequences = False),\n",
    "    Dense(num_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_5.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0005), metrics = ['accuracy'])\n",
    "hist2_5 = model2_5.fit(training2_5, label2_5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ed0aa268",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model2_5'].append(hist2_5.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9e78cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/sentmodel2_5.h5') is False:\n",
    "    model2_5.save('./models/sentmodel2_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44be31",
   "metadata": {},
   "source": [
    "- Senitment Method 3 (with Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "297fb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential([\n",
    "    Dense(units=150, input_shape=(training3.shape[1],), activation='relu'),\n",
    "    Dense(units=200, activation='relu'),\n",
    "    Dense(units=200, activation='relu'),\n",
    "    Dense(units=num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74427efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "hist3 = model3.fit(x=training3, y=label3, validation_split=0.2, batch_size=50, epochs=epochs, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5c7409d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model3'].append(hist3.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "52faf376",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/sentmodel3.h5') is False:\n",
    "    model3.save('./models/sentmodel3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "61750338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset3 combined with SimpleRNN\n",
    "\n",
    "training3_5 = np.array(training3).reshape((training3.shape[0]), training3.shape[1], 1)\n",
    "\n",
    "label3_5 = to_categorical(np.array(sent), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "03b2be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_5 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training3.shape[1], 1), return_sequences = False),\n",
    "    Dense(num_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_5.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "hist3_5 = model3_5.fit(training3_5, label3_5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "44314652",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model3_5'].append(hist3_5.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "30b157b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/sentmodel3_5.h5') is False:\n",
    "    model3_5.save('./models/sentmodel3_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e0425",
   "metadata": {},
   "source": [
    "- Sentiment Method 4 (with Dataset 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "13195f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential([\n",
    "    Dense(units=150, input_shape=(training4.shape[1],), activation='relu'),\n",
    "    Dense(units=200, activation='relu'),\n",
    "    Dense(units=200, activation='relu'),\n",
    "    Dense(units=num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92355859",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "hist4 = model4.fit(x=training4, y=label4, validation_split=0.2, batch_size=50, epochs=epochs, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "99eee689",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model4'].append(hist4.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3ab7f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/sentmodel4.h5') is False:\n",
    "    model4.save('./models/sentmodel4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f17fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset4 combined with SimpleRNN\n",
    "\n",
    "training4_5 = np.array(training4).reshape((training4.shape[0]), training4.shape[1], 1)\n",
    "\n",
    "label4_5 = to_categorical(np.array(sent), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a96649ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_5 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training4.shape[1], 1), return_sequences = False),\n",
    "    Dense(num_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_5.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "hist4_5 = model4_5.fit(training4_5, label4_5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3fac6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model4_5'].append(hist4_5.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7ea148c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/sentmodel4_5.h5') is False:\n",
    "    model4_5.save('./models/sentmodel4_5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "281f38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model4_6 = Sequential([\n",
    "    #LSTM(50, input_shape = (training4.shape[1], 1), return_sequences = False),\n",
    "    #Dense(num_classes, activation='softmax'),\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model4_6.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "#hist4_6 = model4_6.fit(training4_5, label4_5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "89c2aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores_dict['model4_6'].append(hist4_6.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "75dc926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if os.path.isfile('./models/sentmodel4_6.h5') is False:\n",
    "    #model4_6.save('./models/sentmodel4_6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e68ad1",
   "metadata": {},
   "source": [
    "- Sentiment Method 5 (with Dataset 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6ea563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset5 combined with SimpleRNN\n",
    "\n",
    "training5 = tf.convert_to_tensor(training5, dtype=tf.int64)\n",
    "\n",
    "label5 = to_categorical(label5, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "85310cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training5.shape[1], training5.shape[2]), return_sequences = False),\n",
    "    Dense(num_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "hist5 = model5.fit(training5, label5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1c671025",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model5'].append(hist5.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "63a09b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/sentmodel5.h5') is False:\n",
    "    model5.save('./models/sentmodel5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5adc998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model5_5 = Sequential([\n",
    "    #LSTM(50, input_shape = (training5.shape[1], training5.shape[2]), return_sequences = False),\n",
    "    #Dense(num_classes, activation='softmax'),\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a212b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model5_5.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "#hist5_5 = model5_5.fit(training5, label5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e122350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores_dict['model5_5'].append(hist5_5.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d32211db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if os.path.isfile('./models/sentmodel5_5.h5') is False:\n",
    "    #model5_5.save('./models/sentmodel5_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3551f0",
   "metadata": {},
   "source": [
    "- Sentiment Method 6 (with Dataset 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "410aa956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset6 combined with SimpleRNN\n",
    "\n",
    "training6 = np.array(training6).reshape((training6.shape[0]), training6.shape[1], 1)\n",
    "\n",
    "label6 = to_categorical(label6, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b557ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training6.shape[1], 1), return_sequences = False),\n",
    "    Dense(num_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba74b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model6 = Sequential([\n",
    "    #LSTM(50, input_shape = (training6.shape[1], 1), return_sequences = False),\n",
    "    #Dense(num_classes, activation='softmax'),\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61921782",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "hist6 = model6.fit(training6, label6, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8a0c5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict['model6'].append(hist6.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d344528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/sentmodel6.h5') is False:\n",
    "    model6.save('./models/sentmodel6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1763feb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model2_5</th>\n",
       "      <th>model3</th>\n",
       "      <th>model3_5</th>\n",
       "      <th>model4</th>\n",
       "      <th>model4_5</th>\n",
       "      <th>model5</th>\n",
       "      <th>model6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Validation_Accuracy</th>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.552381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model1  model2  model2_5    model3  model3_5    model4  \\\n",
       "Validation_Accuracy  0.511905    0.45  0.607143  0.428571  0.540476  0.595238   \n",
       "\n",
       "                     model4_5    model5    model6  \n",
       "Validation_Accuracy  0.607143  0.607143  0.552381  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = ['Validation_Accuracy']\n",
    "scores_df = pd.DataFrame(scores_dict, index = index)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c15a9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv('./models/sentmodels_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cee4b9",
   "metadata": {},
   "source": [
    "- Loading already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading model from local repository\n",
    "model1 = pickle.load(open('./models/sentmodel1.pkl', 'rb'))\n",
    "model2 = load_model('./models/sentmodel2.h5')\n",
    "model2_5 = load_model('./models/sentmodel2_5.h5')\n",
    "model3 = load_model('./models/sentmodel3.h5')\n",
    "model3_5 = load_model('./models/sentmodel3_5.h5')\n",
    "model4 = load_model('./models/sentmodel4.h5')\n",
    "model4_5 = load_model('./models/sentmodel4_5.h5')\n",
    "model5 = load_model('./models/sentmodel5.h5')\n",
    "model6 = load_model('./models/sentmodel6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c585978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading model from remote repository\n",
    "urllib.request.urlretrieve(\n",
    "        'https://github.com/dub-em/Election-Campaign-Application-Phase2/raw/main/models/damodel8.h5', 'damodel8.h5')\n",
    "\n",
    "link = './damodel8.h5'\n",
    "\n",
    "model = load_model(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38730381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_29 (Dense)            (None, 50)                15050     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 100)               5100      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,856\n",
      "Trainable params: 30,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fff8d2",
   "metadata": {},
   "source": [
    "## Discourse Area Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e145c040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2096, 2)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset for Discourse Area Analysis\n",
    "datweets = tweets[[\"clean_tweet\",\"discourse_area\"]][~tweets.discourse_area.isnull()]\n",
    "datweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4757d24",
   "metadata": {},
   "source": [
    "### Dataset Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ded9cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes2 = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9f9b1",
   "metadata": {},
   "source": [
    "- 7. Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f535d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_area = le.fit_transform(datweets['discourse_area'])\n",
    "datext = datweets['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "02183403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.72 s\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "messages, word_features = bagofwords_preprocessing(datext, disc_area)\n",
    "\n",
    "#building dataset using our function for generating rows using our word features\n",
    "dataset_7 = [(find_features(datext, word_features), label) for (datext, label) in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b1b5b",
   "metadata": {},
   "source": [
    "- 8. Sentence Character-Encoder (Custom Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "6ed55011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 57s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_8 = custom_sentencencoder(datext)\n",
    "dataset_8 = dataset_8.astype(float)\n",
    "\n",
    "training8 = dataset_8.to_numpy()\n",
    "label8 = np.array(disc_area)\n",
    "label8 = to_categorical(label8, num_classes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9e7cc",
   "metadata": {},
   "source": [
    "- 9. Sentence Word-Encoder (GENSIM + PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a3d7c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14min 10s\n",
      "Wall time: 11min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_9 = datext.apply(sent_vect3)\n",
    "dataset_9 = pd.DataFrame(dataset_9.tolist())\n",
    "dataset_9.fillna(0, inplace=True)\n",
    "\n",
    "training9 = dataset_9.to_numpy()\n",
    "label9 = np.array(disc_area)\n",
    "label9 = to_categorical(label9, num_classes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce785af",
   "metadata": {},
   "source": [
    "- 10. Sentence Word-Encoder (GENSIM + Avg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c62ac82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 45s\n",
      "Wall time: 9min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_10 = datext.apply(sent_vect4)\n",
    "dataset_10 = pd.DataFrame(dataset_10.tolist())\n",
    "dataset_10.fillna(0, inplace=True)\n",
    "\n",
    "training10 = dataset_10.to_numpy()\n",
    "label10 = np.array(disc_area)\n",
    "label10 = to_categorical(label10, num_classes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125faa9",
   "metadata": {},
   "source": [
    "- 11. Sentence Word-Encoder (GENSIM to Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "93916742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 58s\n",
      "Wall time: 10min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset_11 = sent_vect5(datext)\n",
    "\n",
    "training11 = np.array(dataset_11)\n",
    "label11 = np.array(disc_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4781ab",
   "metadata": {},
   "source": [
    "- 12. Word Encoder Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "350043dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(num_words=1000, lower=True)\n",
    "tokenizer2.fit_on_texts(datext)\n",
    "\n",
    "x_train2 = tokenizer2.texts_to_sequences(datext)\n",
    "\n",
    "#adding 1 because of reserved 0 index\n",
    "vocab_size2 = len(tokenizer2.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b8dc587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 27.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maxlen = 50 #we set the maximum size of each list to 100\n",
    "\n",
    "training12 = pad_sequences(x_train2, padding='post', maxlen=maxlen)\n",
    "label12 = np.array(disc_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a34345",
   "metadata": {},
   "source": [
    "#### Disourse Area Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1492d21",
   "metadata": {},
   "source": [
    "- National"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "1a6abe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2 = {'model7':[],'model8':[],'model8_5':[],'model9':[],'model9_5':[],'model10':[],'model10_5':[],'model11':[],'model12':[]}\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b811bb",
   "metadata": {},
   "source": [
    "- Disourse Method 7 (with Dataset 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d08381e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training7, testing7 = model_selection.train_test_split(dataset_7, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "03cd5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a model list, just as it is done when creating a pipeline.\n",
    "names = ['Naive Bayes']\n",
    "classifier = [MultinomialNB()]\n",
    "\n",
    "models = dict(zip(names, classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9179097a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(MultinomialNB())>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = SklearnClassifier(models['Naive Bayes'])\n",
    "model7.train(training7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "351a4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model7'].append(nltk.classify.accuracy(model7, testing7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "07a7d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./models/damodel7.pkl\"\n",
    "pickle.dump(model7, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb301954",
   "metadata": {},
   "source": [
    "- Discourse Method 8 (with Dataset 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "126e56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = Sequential([\n",
    "    Dense(units=50, input_shape=(training8.shape[1],), activation='relu'),\n",
    "    Dense(units=100, activation='relu'),\n",
    "    Dense(units=100, activation='relu'),\n",
    "    Dense(units=num_classes2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ec2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model8.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist8 = model8.fit(x=training8, y=label8, validation_split=0.2, batch_size=50, epochs=epochs, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "152f7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model8'].append(hist8.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "13932ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/damodel8.h5') is False:\n",
    "    model8.save('./models/damodel8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8c6addc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset8 combined with SimpleRNN\n",
    "\n",
    "training8_5 = np.array(training8).reshape((training8.shape[0]), training8.shape[1], 1)\n",
    "\n",
    "label8_5 = to_categorical(np.array(disc_area), num_classes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8212c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model8_5 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training8.shape[1], 1), return_sequences = False),\n",
    "    Dense(num_classes2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372577bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model8_5.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0005), metrics = ['accuracy'])\n",
    "hist8_5 = model8_5.fit(training8_5, label8_5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "59dab31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model8_5'].append(hist8_5.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ce0a89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/damodel8_5.h5') is False:\n",
    "    model8_5.save('./models/damodel8_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048dd3d1",
   "metadata": {},
   "source": [
    "- Discourse Method 9 (with Dataset 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "914fca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = Sequential([\n",
    "    Dense(units=150, input_shape=(training9.shape[1],), activation='relu'),\n",
    "    Dense(units=200, activation='relu'),\n",
    "    Dense(units=200, activation='relu'),\n",
    "    Dense(units=num_classes2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de044b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model9.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist9 = model9.fit(x=training9, y=label9, validation_split=0.2, batch_size=50, epochs=epochs, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0bb03c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model9'].append(hist9.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "cf77a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/damodel9.h5') is False:\n",
    "    model9.save('./models/damodel9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "463f5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset9 combined with SimpleRNN\n",
    "\n",
    "training9_5 = np.array(training9).reshape((training9.shape[0]), training9.shape[1], 1)\n",
    "\n",
    "label9_5 = to_categorical(np.array(disc_area), num_classes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "93158b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model9_5 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training9.shape[1], 1), return_sequences = False),\n",
    "    Dense(num_classes2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model9_5.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "hist9_5 = model9_5.fit(training9_5, label9_5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "b39776a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model9_5'].append(hist9_5.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e0703ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/damodel9_5.h5') is False:\n",
    "    model9_5.save('./models/damodel9_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9fd752",
   "metadata": {},
   "source": [
    "- Discourse Method 10 (with Dataset 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "7e2ed618",
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = Sequential([\n",
    "    Dense(units=150, input_shape=(training10.shape[1],), activation='relu'),\n",
    "    Dense(units=200, activation='relu'),\n",
    "    Dense(units=200, activation='relu'),\n",
    "    Dense(units=num_classes2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model10.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist10 = model10.fit(x=training10, y=label10, validation_split=0.2, batch_size=50, epochs=epochs, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "88c8e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model10'].append(hist10.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "21985cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/damodel10.h5') is False:\n",
    "    model10.save('./models/damodel10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "8ddbe4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset10 combined with SimpleRNN\n",
    "\n",
    "training10_5 = np.array(training10).reshape((training10.shape[0]), training10.shape[1], 1)\n",
    "\n",
    "label10_5 = to_categorical(np.array(disc_area), num_classes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "4e9ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model10_5 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training10.shape[1], 1), return_sequences = False),\n",
    "    Dense(num_classes2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d978a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model10_5.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "hist10_5 = model10_5.fit(training10_5, label10_5, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "641d8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model10_5'].append(hist10_5.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c8cd8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/damodel10_5.h5') is False:\n",
    "    model10_5.save('./models/damodel10_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1fa44",
   "metadata": {},
   "source": [
    "- Discourse Method 11 (with Dataset 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "deb33d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset11 combined with SimpleRNN\n",
    "\n",
    "training11 = tf.convert_to_tensor(training11, dtype=tf.int64)\n",
    "\n",
    "label11 = to_categorical(label11, num_classes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "109c9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training11.shape[1], training11.shape[2]), return_sequences = False),\n",
    "    Dense(num_classes2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model11.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "hist11 = model11.fit(training11, label11, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "a8363542",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model11'].append(hist11.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "844588f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/damodel11.h5') is False:\n",
    "    model11.save('./models/damodel11.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814646cb",
   "metadata": {},
   "source": [
    "- Discourse Method 12 (with Dataset 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "a74f162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset12 combined with SimpleRNN\n",
    "\n",
    "training12 = np.array(training12).reshape((training12.shape[0]), training12.shape[1], 1)\n",
    "\n",
    "label12 = to_categorical(label12, num_classes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "ad50edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = Sequential([\n",
    "    SimpleRNN(50, input_shape = (training12.shape[1], 1), return_sequences = False),\n",
    "    Dense(num_classes2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ec1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model12.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate=0.0001), metrics = ['accuracy'])\n",
    "hist12 = model12.fit(training12, label12, epochs = epochs, batch_size = 50, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9f7b5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict2['model12'].append(hist12.history[\"val_accuracy\"][epochs - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "8b0690d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('./models/damodel12.h5') is False:\n",
    "    model12.save('./models/damodel12.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "ce0eabcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model7</th>\n",
       "      <th>model8</th>\n",
       "      <th>model8_5</th>\n",
       "      <th>model9</th>\n",
       "      <th>model9_5</th>\n",
       "      <th>model10</th>\n",
       "      <th>model10_5</th>\n",
       "      <th>model11</th>\n",
       "      <th>model12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Validation_Accuracy</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model7  model8  model8_5  model9  model9_5  model10  \\\n",
       "Validation_Accuracy  0.942857     1.0       1.0     1.0       1.0      1.0   \n",
       "\n",
       "                     model10_5  model11  model12  \n",
       "Validation_Accuracy        1.0      1.0      1.0  "
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df2 = pd.DataFrame(scores_dict2, index = index)\n",
    "scores_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "bf277219",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df2.to_csv('./models/damodels_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2f4c0",
   "metadata": {},
   "source": [
    "- Loading already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe872a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pickle.load(open('./models/sentmodel1.pkl', 'rb'))\n",
    "model2 = load_model('./models/sentmodel2.h5')\n",
    "model2_5 = load_model('./models/sentmodel2_5.h5')\n",
    "model3 = load_model('./models/sentmodel3.h5')\n",
    "model3_5 = load_model('./models/sentmodel3_5.h5')\n",
    "model4 = load_model('./models/sentmodel4.h5')\n",
    "model4_5 = load_model('./models/sentmodel4_5.h5')\n",
    "model5 = load_model('./models/sentmodel5.h5')\n",
    "model6 = load_model('./models/sentmodel6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1a731",
   "metadata": {},
   "source": [
    "#### GENSIM LDA Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e184e8b",
   "metadata": {},
   "source": [
    "- https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "af1572be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def sent_to_word(sentences):\n",
    "    \"\"\" This function removes stop words usign a different method from the precious one(s) used.\"\"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def LDA_parameters(text):\n",
    "    \"\"\"This function takes the text, processes it, and return the parameters to build the LDA Topic Model.\"\"\"\n",
    "    \n",
    "    data = text.tolist()\n",
    "    data_words = list(sent_to_word(data))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    #trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words, bigram_mod)\n",
    "    data_words_bigrams[:1]\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    return corpus, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9675551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.3 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus, id2word = LDA_parameters(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c482e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.61 s\n",
      "Wall time: 7.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word,\n",
    "                                           num_topics=10, random_state=100,\n",
    "                                           update_every=1, chunksize=100,\n",
    "                                           passes=10, alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "09428e00",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.058*\"know\" + 0.052*\"well\" + 0.037*\"flood\" + 0.036*\"do\" + 0.027*\"use\" + 0.022*\"bad\" + 0.017*\"many\" + 0.013*\"pray\" + 0.013*\"issue\" + 0.011*\"speak\"'),\n",
       " (1,\n",
       "  '0.057*\"leave\" + 0.031*\"start\" + 0.029*\"oil\" + 0.026*\"buhari\" + 0.021*\"child\" + 0.019*\"office\" + 0.018*\"accuse\" + 0.017*\"dele_momodu\" + 0.012*\"human\" + 0.011*\"position\"'),\n",
       " (2,\n",
       "  '0.061*\"make\" + 0.039*\"time\" + 0.037*\"want\" + 0.021*\"first\" + 0.020*\"call\" + 0.020*\"manifesto\" + 0.017*\"money\" + 0.016*\"all\" + 0.016*\"look\" + 0.013*\"anambra\"'),\n",
       " (3,\n",
       "  '0.059*\"more\" + 0.043*\"government\" + 0.034*\"bring\" + 0.029*\"re\" + 0.028*\"seyi\" + 0.024*\"promise\" + 0.020*\"attack\" + 0.018*\"always\" + 0.017*\"also\" + 0.017*\"failure\"'),\n",
       " (4,\n",
       "  '0.039*\"s\" + 0.035*\"tell\" + 0.035*\"kano\" + 0.035*\"think\" + 0.034*\"thing\" + 0.022*\"very\" + 0.017*\"visit\" + 0.017*\"here\" + 0.014*\"obi\" + 0.013*\"continue\"'),\n",
       " (5,\n",
       "  '0.047*\"nigerian\" + 0.045*\"support\" + 0.039*\"work\" + 0.026*\"help\" + 0.023*\"right\" + 0.021*\"follow\" + 0.021*\"word\" + 0.018*\"head\" + 0.017*\"father\" + 0.017*\"rice\"'),\n",
       " (6,\n",
       "  '0.075*\"apc\" + 0.043*\"say\" + 0.037*\"vote\" + 0.030*\"lago\" + 0.027*\"now\" + 0.024*\"campaign\" + 0.022*\"president\" + 0.021*\"candidate\" + 0.020*\"win\" + 0.019*\"even\"'),\n",
       " (7,\n",
       "  '0.077*\"just\" + 0.032*\"governor\" + 0.028*\"other\" + 0.024*\"same\" + 0.017*\"problem\" + 0.016*\"stand\" + 0.016*\"too\" + 0.015*\"sir\" + 0.013*\"show\" + 0.012*\"claim\"'),\n",
       " (8,\n",
       "  '0.062*\"go\" + 0.052*\"see\" + 0.044*\"come\" + 0.033*\"man\" + 0.032*\"get\" + 0.024*\"take\" + 0.024*\"lie\" + 0.020*\"country\" + 0.018*\"need\" + 0.017*\"back\"'),\n",
       " (9,\n",
       "  '0.084*\"atiku\" + 0.057*\"pdp\" + 0.052*\"people\" + 0.046*\"state\" + 0.042*\"so\" + 0.027*\"give\" + 0.019*\"party\" + 0.016*\"one\" + 0.015*\"leader\" + 0.015*\"again\"')]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()\n",
    "\n",
    "#doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9c00b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f98e8f",
   "metadata": {},
   "source": [
    "### General Trends "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f346bc6",
   "metadata": {},
   "source": [
    "- This sections covers the generic trends existing amongst citizens' discussion groups.\n",
    "        - What is most talked about (regardless of area or topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5f8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3676b8d",
   "metadata": {},
   "source": [
    "### Citizens' Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e2e720",
   "metadata": {},
   "source": [
    "- This section covers citizens' reactions and general sentiment towards certain topic (e.g areas of developments, policies, politically significant events, public office holders' performance and so on).\n",
    "        - What is the general sentiment of the citizens?\n",
    "\t\t- What is most discussed(election and governance related)?\n",
    "\t\t- What is the sentiment towards what is being discussed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0328120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482ce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ded69742",
   "metadata": {},
   "source": [
    "### Complaint Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf0c6f",
   "metadata": {},
   "source": [
    "- This section covers the extraction of various areas of complaints and dissatisfaction amongst citizens (in different aspects of government).\n",
    "        - What are the various areas of complaints as regards to governance?\n",
    "\t\t- What are the levels of sentiment towards the various area of complaints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72bf324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379e29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d1cc57b",
   "metadata": {},
   "source": [
    "### Politician's Reputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b925500",
   "metadata": {},
   "source": [
    "- This section covers what citizens's think about certain public office holders, their sentiment towards these individuals and their general popularity or notoriety.\n",
    "        - Who is most talked about?\n",
    "\t\t- Popularity or notoreity of the most talked about.\n",
    "\t\t- Most popular, and most notorious candidates/politician.\n",
    "\t\t- How much is a certain candidate being talked about?\n",
    "\t\t- What is being said about each candidate?\n",
    "\t\t- What is the general sentiment of what is being said?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23270b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183bd86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
